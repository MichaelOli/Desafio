# Solu√ß√£o do Desafio 2: Data Lake e Pipeline de APIs

## Vis√£o Geral

Este documento apresenta a solu√ß√£o completa para o **Desafio 2**, que aborda o armazenamento e gerenciamento de dados de APIs de restaurante em um data lake, incluindo estrat√©gias para evolu√ß√£o de schema e opera√ß√µes em escala.

## 1. Por que Armazenar as Respostas das APIs?

### Justificativas T√©cnicas:

#### 1.1 **Auditoria e Compliance**
- **Rastreabilidade Completa**: Hist√≥rico de todas as transa√ß√µes
- **Conformidade Fiscal**: Atendimento √†s exig√™ncias da Receita Federal
- **Auditoria Interna**: Verifica√ß√£o de processos e controles

#### 1.2 **An√°lise e Business Intelligence**
- **Dados Hist√≥ricos**: Tend√™ncias e padr√µes de longo prazo
- **An√°lise Preditiva**: Machine learning sobre dados hist√≥ricos
- **Relat√≥rios Regulat√≥rios**: Compliance com √≥rg√£os fiscalizadores

#### 1.3 **Recupera√ß√£o e Continuidade**
- **Disaster Recovery**: Reconstru√ß√£o de dados em caso de falhas
- **Backup Operacional**: Redund√¢ncia para sistemas cr√≠ticos
- **Reconcilia√ß√£o**: Verifica√ß√£o de consist√™ncia entre sistemas

#### 1.4 **Integra√ß√£o e ETL**
- **Fonte de Verdade**: Dados originais para processamento
- **Reprocessamento**: Capacidade de reexecutar pipelines
- **Integra√ß√£o Futura**: Novos sistemas podem acessar dados hist√≥ricos

### Benef√≠cios de Neg√≥cio:

- **Insights Avan√ßados**: An√°lises que n√£o s√£o poss√≠veis em tempo real
- **Otimiza√ß√£o Operacional**: Identifica√ß√£o de gargalos e oportunidades
- **Conformidade Legal**: Atendimento a regulamenta√ß√µes
- **Redu√ß√£o de Riscos**: Backup e auditoria completos

## 2. Estrutura de Armazenamento do Data Lake

### 2.1 Arquitetura Geral

```
dados/data_lake/
‚îú‚îÄ‚îÄ dados_brutos/           # Raw data from APIs
‚îÇ   ‚îú‚îÄ‚îÄ bilgetFiscalInvoice/
‚îÇ   ‚îú‚îÄ‚îÄ getGuestChecks/
‚îÇ   ‚îú‚îÄ‚îÄ getChargeBack/
‚îÇ   ‚îú‚îÄ‚îÄ getTransactions/
‚îÇ   ‚îî‚îÄ‚îÄ getCashManagementDetails/
‚îú‚îÄ‚îÄ dados_processados/      # Processed/transformed data
‚îú‚îÄ‚îÄ esquemas/              # Schema definitions and versions
‚îú‚îÄ‚îÄ metadados/            # Metadata for quick queries
‚îú‚îÄ‚îÄ arquivo/              # Archived old data
‚îî‚îÄ‚îÄ temporario/           # Temporary processing files
```

### 2.2 Estrat√©gia de Particionamento

#### Particionamento Hier√°rquico por Data e Loja:
```
dados_brutos/getGuestChecks/
‚îú‚îÄ‚îÄ ano=2024/
‚îÇ   ‚îú‚îÄ‚îÄ mes=01/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dia=15/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loja=loja001/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ getGuestChecks_loja001_20240115_143022.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ getGuestChecks_loja001_20240115_143055.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ loja=loja002/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dia=16/
‚îÇ   ‚îî‚îÄ‚îÄ mes=02/
‚îî‚îÄ‚îÄ ano=2025/
```

#### Vantagens do Particionamento:

1. **Performance de Consulta**:
   - Elimina√ß√£o de parti√ß√µes irrelevantes (partition pruning)
   - Paraleliza√ß√£o de consultas por parti√ß√£o
   - √çndices menores e mais eficientes

2. **Gerenciamento de Dados**:
   - Arquivamento seletivo por per√≠odo
   - Backup incremental por parti√ß√£o
   - Limpeza autom√°tica de dados antigos

3. **Escalabilidade**:
   - Distribui√ß√£o de carga por parti√ß√µes
   - Processamento paralelo
   - Crescimento linear de performance

### 2.3 Formato de Armazenamento

#### Estrutura do Arquivo JSON:
```json
{
  "metadados": {
    "endpoint": "getGuestChecks",
    "data_negocio": "2024-01-15",
    "id_loja": "loja001",
    "timestamp_ingestao": "2024-01-15T14:30:22.123Z",
    "versao_esquema": "1.0",
    "hash_dados": "a1b2c3d4e5f6...",
    "tamanho_bytes": 2048,
    "origem": "sistema_pos",
    "usuario": "operador001"
  },
  "dados": {
    // Dados originais da API
  }
}
```

#### Benef√≠cios do Formato:

1. **Metadados Ricos**: Contexto completo para cada arquivo
2. **Versionamento**: Controle de evolu√ß√£o de schema
3. **Auditoria**: Rastreabilidade completa
4. **Integridade**: Hash para verifica√ß√£o de dados

## 3. Tratamento de Evolu√ß√£o de Schema

### 3.1 Cen√°rio: Mudan√ßa de `guestChecks.taxes` para `guestChecks.taxation`

#### Impactos Identificados:

1. **Quebra de Compatibilidade**:
   - Pipelines existentes falhar√£o
   - Consultas SQL precisar√£o ser atualizadas
   - Dashboards podem parar de funcionar

2. **Dados Hist√≥ricos**:
   - Arquivos antigos mant√™m estrutura original
   - Necessidade de mapeamento entre vers√µes
   - Consultas unificadas se tornam complexas

3. **Sistemas Downstream**:
   - ETL processes precisam ser atualizados
   - Data warehouses requerem altera√ß√µes
   - APIs de consumo podem quebrar

### 3.2 Estrat√©gia de Mitiga√ß√£o

#### 3.2.1 Versionamento de Schema

```python
class GerenciadorEvolucaoEsquema:
    def __init__(self):
        self.versoes_esquema = {
            'getGuestChecks': {
                '1.0': {'campo_imposto': 'taxes'},
                '1.1': {'campo_imposto': 'taxation'}
            }
        }
    
    def normalizar_dados(self, endpoint, dados, versao_origem):
        """Converte dados para vers√£o mais recente"""
        if versao_origem == '1.0' and 'taxes' in dados:
            dados['taxation'] = dados.pop('taxes')
        return dados
```

#### 3.2.2 Camada de Abstra√ß√£o

```python
class AdaptadorEsquema:
    def obter_campo_imposto(self, dados, versao_esquema):
        """Abstrai diferen√ßas entre vers√µes"""
        mapeamento = {
            '1.0': 'taxes',
            '1.1': 'taxation'
        }
        campo = mapeamento.get(versao_esquema, 'taxation')
        return dados.get(campo, [])
```

#### 3.2.3 Pipeline de Migra√ß√£o

```python
def migrar_dados_historicos():
    """Migra dados antigos para nova estrutura"""
    for arquivo in buscar_arquivos_versao_antiga():
        dados = carregar_dados(arquivo)
        dados_migrados = aplicar_migracao(dados)
        salvar_dados_migrados(dados_migrados)
        marcar_como_migrado(arquivo)
```

### 3.3 Implementa√ß√£o da Solu√ß√£o

#### 3.3.1 Detec√ß√£o Autom√°tica de Mudan√ßas

```python
class DetectorMudancasEsquema:
    def detectar_mudancas(self, dados_novos, esquema_atual):
        """Detecta mudan√ßas no schema automaticamente"""
        mudancas = []
        
        # Campos removidos
        for campo in esquema_atual:
            if campo not in dados_novos:
                mudancas.append(f"Campo removido: {campo}")
        
        # Campos adicionados
        for campo in dados_novos:
            if campo not in esquema_atual:
                mudancas.append(f"Campo adicionado: {campo}")
        
        return mudancas
```

#### 3.3.2 Notifica√ß√£o de Mudan√ßas

```python
class NotificadorMudancas:
    def notificar_mudanca_esquema(self, endpoint, mudancas):
        """Notifica equipes sobre mudan√ßas de schema"""
        mensagem = f"""
        üö® MUDAN√áA DE SCHEMA DETECTADA
        
        Endpoint: {endpoint}
        Mudan√ßas: {mudancas}
        A√ß√£o Requerida: Atualizar pipelines downstream
        """
        self.enviar_alerta(mensagem)
```

#### 3.3.3 Versionamento Autom√°tico

```python
class GerenciadorVersaoEsquema:
    def criar_nova_versao(self, endpoint, esquema_novo):
        """Cria nova vers√£o do schema automaticamente"""
        versao_atual = self.obter_versao_atual(endpoint)
        nova_versao = self.incrementar_versao(versao_atual)
        
        self.salvar_esquema(endpoint, nova_versao, esquema_novo)
        self.atualizar_mapeamentos(endpoint, nova_versao)
        
        return nova_versao
```

## 4. Opera√ß√µes e Manuten√ß√£o

### 4.1 Monitoramento

#### M√©tricas Principais:
- **Volume de Dados**: GB ingeridos por dia/endpoint
- **Lat√™ncia**: Tempo entre API call e armazenamento
- **Qualidade**: Percentual de arquivos com schema v√°lido
- **Disponibilidade**: Uptime do sistema de ingest√£o

#### Alertas Configurados:
- Falha na ingest√£o de dados
- Mudan√ßa de schema detectada
- Espa√ßo em disco baixo
- Performance degradada

### 4.2 Backup e Arquivamento

#### Estrat√©gia de Backup:
```python
class GerenciadorBackup:
    def criar_backup_incremental(self):
        """Backup di√°rio de dados novos"""
        data_ultimo_backup = self.obter_data_ultimo_backup()
        arquivos_novos = self.buscar_arquivos_desde(data_ultimo_backup)
        self.copiar_para_backup(arquivos_novos)
    
    def criar_backup_completo(self):
        """Backup semanal completo"""
        self.copiar_data_lake_completo()
```

#### Pol√≠tica de Reten√ß√£o:
- **Dados Ativos**: 90 dias no storage principal
- **Dados Arquivados**: 7 anos em storage frio
- **Backups**: 30 dias backup incremental, 1 ano backup completo

### 4.3 Limpeza Autom√°tica

```python
class LimpadorDados:
    def limpar_dados_antigos(self, dias_retencao=90):
        """Remove dados mais antigos que per√≠odo de reten√ß√£o"""
        data_corte = datetime.now() - timedelta(days=dias_retencao)
        
        for pasta_data in self.listar_pastas_por_data():
            if pasta_data.data < data_corte:
                self.arquivar_pasta(pasta_data)
                self.remover_pasta_local(pasta_data)
```

## 5. Consultas e An√°lises

### 5.1 Interface de Consulta

```python
class ConsultorDataLake:
    def consultar_vendas_periodo(self, data_inicio, data_fim, loja=None):
        """Consulta vendas por per√≠odo"""
        return self.gerenciador.buscar_dados(
            endpoint="getGuestChecks",
            data_inicio=data_inicio,
            data_fim=data_fim,
            id_loja=loja
        )
    
    def obter_metricas_loja(self, id_loja, periodo):
        """M√©tricas consolidadas de uma loja"""
        dados = self.consultar_todos_endpoints(id_loja, periodo)
        return self.calcular_metricas(dados)
```

### 5.2 Integra√ß√£o com Ferramentas de BI

```python
class ConectorBI:
    def exportar_para_parquet(self, consulta):
        """Converte dados JSON para Parquet para BI tools"""
        dados = self.executar_consulta(consulta)
        df = pd.DataFrame(dados)
        return df.to_parquet()
    
    def criar_view_sql(self, nome_view, consulta):
        """Cria view SQL para acesso via BI tools"""
        self.executar_ddl(f"CREATE VIEW {nome_view} AS {consulta}")
```

## 6. Benef√≠cios da Solu√ß√£o Implementada

### 6.1 T√©cnicos:
‚úÖ **Escalabilidade**: Particionamento permite crescimento linear

‚úÖ **Performance**: Consultas otimizadas por elimina√ß√£o de parti√ß√µes

‚úÖ **Flexibilidade**: Suporte a evolu√ß√£o de schema sem perda de dados

‚úÖ **Confiabilidade**: Backup, monitoramento e recupera√ß√£o autom√°ticos

### 6.2 Operacionais:
‚úÖ **Auditoria Completa**: Rastreabilidade de todas as opera√ß√µes

‚úÖ **Compliance**: Atendimento a regulamenta√ß√µes fiscais

‚úÖ **An√°lise Avan√ßada**: Dados hist√≥ricos para BI e ML

‚úÖ **Recupera√ß√£o R√°pida**: Restore de dados em caso de falhas

### 6.3 Estrat√©gicos:
‚úÖ **Fonte de Verdade**: Dados originais preservados

‚úÖ **Inova√ß√£o**: Base para novos produtos e an√°lises

‚úÖ **Competitividade**: Insights √∫nicos sobre opera√ß√µes

‚úÖ **Crescimento**: Infraestrutura preparada para expans√£o

## 7. Conclus√£o

A solu√ß√£o de data lake implementada oferece uma base s√≥lida e escal√°vel para o armazenamento e an√°lise de dados de APIs de restaurante. A estrat√©gia de particionamento, versionamento de schema e opera√ß√µes automatizadas garante que o sistema seja robusto, eficiente e preparado para o crescimento futuro do neg√≥cio.

A abordagem adotada equilibra as necessidades de performance, flexibilidade e governan√ßa, resultando em uma plataforma de dados moderna e adequada para opera√ß√µes empresariais em escala.